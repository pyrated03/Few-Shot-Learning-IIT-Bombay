{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Noise_Introduction RNNP(SUB 3_part 1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVYxql8yBGr+krmNFs5NvH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyrated03/Few-Shot-Learning-IIT-Bombay/blob/main/Noise_Introduction_RNNP(SUB_3_part_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrxLV2BHQs6f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u7XvMCIvc8h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import os.path as osp\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import random\n",
        "import math\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58KJlemiuxWf"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained model\n",
        "model_vec = models.resnet18(pretrained=True)\n",
        "# Use the model object to select the desired layer\n",
        "layer = model_vec._modules.get('avgpool')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr2wOtrCvTBY",
        "outputId": "6a9958bc-8589-4977-85fb-64182d458c97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# Set model to evaluation mode\n",
        "model_vec.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muy2_NDovWC9",
        "outputId": "709012ed-6882-4b4c-89d0-9d9a5de724fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:317: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ]
        }
      ],
      "source": [
        "scaler = transforms.Scale((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2i3gsP3vXHr"
      },
      "outputs": [],
      "source": [
        "def get_vector(img):\n",
        "    # 1. Load the image with Pillow library\n",
        "    # img = Image.open(image_name)\n",
        "    # 2. Create a PyTorch Variable with the transformed image\n",
        "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
        "    # 3. Create a vector of zeros that will hold our feature vector\n",
        "    #    The 'avgpool' layer has an output size of 512\n",
        "    my_embedding = torch.zeros(512)\n",
        "    # 4. Define a function that will copy the output of a layer\n",
        "    def copy_data(m, i, o):\n",
        "      my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    # 5. Attach that function to our selected layer\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    # 6. Run the model on our transformed image\n",
        "    model_vec(t_img)\n",
        "    # 7. Detach our copy function from the layer\n",
        "    h.remove()\n",
        "    # 8. Return the feature vector\n",
        "    return my_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx2Q59oAZzLl"
      },
      "outputs": [],
      "source": [
        "class CUB2002011_ds(Dataset):\n",
        "    def __init__(self, data_dir, mode=\"train\", noise=0.000001):\n",
        "        assert mode in [\"train\", \"test\"] #Make sure the mode is one of the two\n",
        "        super(CUB2002011_ds, self).__init__() #Inheritance dataset __init__()\n",
        "        train_test_split = pd.read_csv(\n",
        "            f\"{data_dir}/train_test_split.txt\",\n",
        "            sep=\" \",\n",
        "            names=[\"img_id\", \"is_train\"],\n",
        "        )\n",
        "        img_paths = pd.read_csv(\n",
        "            f\"{data_dir}/images.txt\", sep=\" \", names=[\"img_id\", \"img_path\"]\n",
        "        )  #Read all image paths\n",
        "        labels = pd.read_csv(\n",
        "            f\"{data_dir}/image_class_labels.txt\",\n",
        "            sep=\" \",\n",
        "            names=[\"img_id\", \"img_label\"],\n",
        "        )\n",
        "        self.noise = noise\n",
        "        self.data_dir = osp.join(data_dir, \"images\")\n",
        "        self.mode = 1 if mode == \"train\" else 0  #Choose whether it is 1 training mode or test mode 0\n",
        "        self.img_path = img_paths.loc[\n",
        "            train_test_split[\"is_train\"] == self.mode, \"img_path\"\n",
        "        ].values  #Get the corresponding path in the corresponding mode\n",
        "        self.label = labels.loc[\n",
        "            train_test_split[\"is_train\"] == self.mode, \"img_label\"\n",
        "        ].values #Get the label in the corresponding mode\n",
        "        img_sz = 84\n",
        "        if mode == 'test':\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((img_sz, img_sz)),\n",
        "                transforms.ToTensor(),\n",
        "                # transforms.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225))\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((img_sz, img_sz)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(30),\n",
        "                transforms.ToTensor(),\n",
        "                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        self.y = self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.label.shape[0] #The size of the entire data set in the corresponding mode\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.transform(\n",
        "            Image.open(f\"{self.data_dir}/{self.img_path[idx]}\").convert(\"RGB\")\n",
        "        ) #Open the path of the image in the corresponding folder of the data set and convert the image data to RGB\n",
        "        label = self.label[idx]\n",
        "        label = label-1\n",
        "        # NOISE INTRODUCTION\n",
        "        if self.mode == 1:\n",
        "          if idx % (math.ceil(1/self.noise)) == 0:\n",
        "            ran = label\n",
        "            while(ran == label):\n",
        "              ran = random.randint(0,199)\n",
        "            \n",
        "            label = ran\n",
        "        return img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTvH1N5SrBqx"
      },
      "outputs": [],
      "source": [
        "CUB_ds = CUB2002011_ds(\"/content/drive/MyDrive/IITB Internship/CUB_200_2011\", \"train\")#,0.2)\n",
        "train_CUB = torch.utils.data.DataLoader(CUB_ds, batch_size=54, shuffle=False, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PBfAmVFOOC8"
      },
      "outputs": [],
      "source": [
        "train_CUB_length = len(train_CUB)*54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgDjTIgttc_c"
      },
      "outputs": [],
      "source": [
        "CUB_dvec_master = {}\n",
        "X_orig = []\n",
        "for images, labels in train_CUB:\n",
        "  for i in range(54):\n",
        "    labels = np.array(labels)\n",
        "    x_i = (get_vector(transforms.ToPILImage()(images[i]))).numpy()\n",
        "    X_orig.append(x_i)\n",
        "    if labels[i] not in CUB_dvec_master:\n",
        "      CUB_dvec_master[labels[i]] = []\n",
        "      CUB_dvec_master[labels[i]].append(x_i)\n",
        "    else:\n",
        "      CUB_dvec_master[labels[i]].append(x_i)\n",
        "    # print(type(images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNIwkP33ygA0"
      },
      "outputs": [],
      "source": [
        "def augment(L,orig_data, alpha, beta):\n",
        "  for i in range(beta):\n",
        "    im1, im2 = random.sample(orig_data, 2)\n",
        "    L.append(im1 + (alpha)*im2)\n",
        "          \n",
        "  return L\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5Fl-cKWxK7N"
      },
      "outputs": [],
      "source": [
        "Unlabelled_Aug = []\n",
        "for i in range(200):\n",
        "  Unlabelled_Aug = augment(Unlabelled_Aug, CUB_dvec_master[i], 0.25, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC678JmluUxS"
      },
      "source": [
        "Merging to make the feature matrix to train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4mjWBeIslZK"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "for i in range(200):\n",
        "  X = X + CUB_dvec_master[i]\n",
        "\n",
        "X_aug = X + Unlabelled_Aug\n",
        "Y = []\n",
        "for i in range(200):\n",
        "  list_labels = [i] * len(CUB_dvec_master[i])\n",
        "  Y = Y + list_labels\n",
        "# random.shuffle(Y)\n",
        "list_unlabelled = [-1] * len(Unlabelled_Aug)\n",
        "Y_aug = Y + list_unlabelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCTLa1Y07pLj"
      },
      "outputs": [],
      "source": [
        "temp = list(zip(X_aug, Y_aug))\n",
        "random.shuffle(temp)\n",
        "X_fin, Y_fin = zip(*temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqSXfJA58nlM",
        "outputId": "b20a12d8-4fd3-43aa-acb1-d2033a030c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of iteration 1, added 6 new labels.\n"
          ]
        }
      ],
      "source": [
        "# svc = SVC(probability=True, gamma=\"auto\")\n",
        "# lr = LogisticRegression(max_iter=1000)\n",
        "model_svc = SVC(kernel='rbf', \n",
        "                probability=True, # Need to enable to be able to use predict_proba\n",
        "                C=1.5, # default = 1.0\n",
        "                gamma='scale', # default = 'scale',\n",
        "                random_state=0\n",
        "               )\n",
        "# self_training_model = SelfTrainingClassifier(lr, threshold=0.6) #criterion = \"k_best\")\n",
        "self_training_model = SelfTrainingClassifier(base_estimator=model_svc, # An estimator object implementing fit and predict_proba.\n",
        "                                             threshold=0.8, # default=0.75, The decision threshold for use with criterion='threshold'. Should be in [0, 1).\n",
        "                                             criterion='threshold', # {‘threshold’, ‘k_best’}, default=’threshold’, The selection criterion used to select which labels to add to the training set. If 'threshold', pseudo-labels with prediction probabilities above threshold are added to the dataset. If 'k_best', the k_best pseudo-labels with highest prediction probabilities are added to the dataset.\n",
        "                                             #k_best=50, # default=10, The amount of samples to add in each iteration. Only used when criterion='k_best'.\n",
        "                                             max_iter=None, # default=10, Maximum number of iterations allowed. Should be greater than or equal to 0. If it is None, the classifier will continue to predict labels until no new pseudo-labels are added, or all unlabeled samples have been labeled.\n",
        "                                             verbose=True # default=False, Verbosity prints some information after each iteration\n",
        "                                            )\n",
        "clf_ST = self_training_model.fit(X_fin, Y_fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3weemiZN4ye",
        "outputId": "756617cf-1171-4cdc-aae2-408c8c4ec28c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8293293293293293"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "Y_temp = clf_ST.predict(X)\n",
        "accuracy_score(Y, Y_temp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_orig = clf_ST.predict(X_orig)"
      ],
      "metadata": {
        "id": "c04PHbp86OzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_orig_ar = np.array(Y_orig) + 1\n",
        "# file = open(\"/content/drive/MyDrive/IITB Internship/CUB_200_2011/Y_After_Noise.txt\", \"w+\")\n",
        " \n",
        "# Saving the array in a text file\n",
        "# content = str(Y_orig)\n",
        "# file.write(content)\n",
        "# file.close()\n",
        "np.savetxt(\"/content/drive/MyDrive/IITB Internship/CUB_200_2011/Y_After_Noise.txt\", y_orig_ar)"
      ],
      "metadata": {
        "id": "OZqJYTSx_7yV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}