{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUB_5_Shot_Learning_CBAM_with_noise_RNNP(SUB 3 part 2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyrated03/Few-Shot-Learning-IIT-Bombay/blob/main/CUB_5_Shot_Learning_CBAM_with_noise_RNNP(SUB_3_part_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YznmdbdILYX",
        "outputId": "facb283d-6bd1-47f1-8cfc-d0f33473a7d6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNlfrhiZYDl-"
      },
      "source": [
        "# !pip install gevent --pre\n",
        "# !pip install auto-py-to-exe\n",
        "# !pip install -U setuptools\n",
        "# !pip install git+https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXuE7jF7DYCw"
      },
      "source": [
        "# !cp --gdrive/My\\ Drive/IITB\\ Internship/src/Prototypical-Networks-for-Few-shot-Learning-PyTorch/src/prototypical_batch_sampler.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U7Xwn-dDX_3"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/IITB Internship/src/Prototypical-Networks-for-Few-shot-Learning-PyTorch/src/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoq9J9f0EKcc"
      },
      "source": [
        "# from prototypical_batch_sampler import PrototypicalBatchSampler\n",
        "# from prototypical_loss import prototypical_loss as loss_fn\n",
        "# from omniglot_dataset import OmniglotDataset\n",
        "# from protonet import ProtoNet\n",
        "# from parser_util import get_parser\n",
        "# import prototypical_batch_sampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S4pM77pc4Hl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6bab039-4606-4f8d-f600-55419bafcd7c"
      },
      "source": [
        "%%writefile resnet12.py\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# This ResNet network was designed following the practice of the following papers:\n",
        "# TADAM: Task dependent adaptive metric for improved few-shot learning (Oreshkin et al., in NIPS 2018) and\n",
        "# A Simple Neural Attentive Meta-Learner (Mishra et al., in ICLR 2018).\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "##################################################################################################################################################\n",
        "##################################################################################################################################################\n",
        "class CBAM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_channels_in, reduction_ratio, kernel_size):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.n_channels_in = n_channels_in\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.channel_attention = ChannelAttention(n_channels_in, reduction_ratio)\n",
        "        self.spatial_attention = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, f):\n",
        "        chan_att = self.channel_attention(f)\n",
        "        # print(chan_att.size())\n",
        "        fp = chan_att * f\n",
        "        # print(fp.size())\n",
        "        spat_att = self.spatial_attention(fp)\n",
        "        # print(spat_att.size())\n",
        "        fpp = spat_att * fp\n",
        "        # print(fpp.size())\n",
        "        return fpp\n",
        "\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        assert kernel_size % 2 == 1, \"Odd kernel size required\"\n",
        "        self.conv = nn.Conv2d(in_channels = 2, out_channels = 1, kernel_size = kernel_size, padding= int((kernel_size-1)/2))\n",
        "        # batchnorm\n",
        "\n",
        "    def forward(self, x):\n",
        "        max_pool = self.agg_channel(x, \"max\")\n",
        "        avg_pool = self.agg_channel(x, \"avg\")\n",
        "        pool = torch.cat([max_pool, avg_pool], dim = 1)\n",
        "        conv = self.conv(pool)\n",
        "        conv = conv.repeat(1,x.size()[1],1,1)\n",
        "        att = torch.sigmoid(conv)        \n",
        "        return att\n",
        "\n",
        "    def agg_channel(self, x, pool = \"max\"):\n",
        "        b,c,h,w = x.size()\n",
        "        x = x.view(b, c, h*w)\n",
        "        x = x.permute(0,2,1)\n",
        "        if pool == \"max\":\n",
        "            x = F.max_pool1d(x,c)\n",
        "        elif pool == \"avg\":\n",
        "            x = F.avg_pool1d(x,c)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = x.view(b,1,h,w)\n",
        "        return x\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, n_channels_in, reduction_ratio):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.n_channels_in = n_channels_in\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.middle_layer_size = int(self.n_channels_in/ float(self.reduction_ratio))\n",
        "\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Linear(self.n_channels_in, self.middle_layer_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.middle_layer_size, self.n_channels_in)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        kernel = (x.size()[2], x.size()[3])\n",
        "        avg_pool = F.avg_pool2d(x, kernel )\n",
        "        max_pool = F.max_pool2d(x, kernel)\n",
        "\n",
        "        \n",
        "        avg_pool = avg_pool.view(avg_pool.size()[0], -1)\n",
        "        max_pool = max_pool.view(max_pool.size()[0], -1)\n",
        "        \n",
        "\n",
        "        avg_pool_bck = self.bottleneck(avg_pool)\n",
        "        max_pool_bck = self.bottleneck(max_pool)\n",
        "\n",
        "        pool_sum = avg_pool_bck + max_pool_bck\n",
        "\n",
        "        sig_pool = torch.sigmoid(pool_sum)\n",
        "        sig_pool = sig_pool.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        out = sig_pool.repeat(1,1,kernel[0], kernel[1])\n",
        "        return out\n",
        "##################################################################################################################################################\n",
        "##################################################################################################################################################\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, drop_rate=0.0, drop_block=False, block_size=1, reduction_ratio = 1, kernel_cbam = 3, use_cbam=True):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.use_cbam = use_cbam\n",
        "        self.conv1 = conv3x3(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.LeakyReLU(0.1)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv3x3(planes, planes)\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.maxpool = nn.MaxPool2d(stride) \n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.drop_rate = drop_rate\n",
        "        self.num_batches_tracked = 0\n",
        "        self.drop_block = drop_block\n",
        "        self.block_size = block_size\n",
        "\n",
        "        if self.use_cbam:\n",
        "            self.cbam = CBAM(n_channels_in = self.expansion*planes, reduction_ratio = reduction_ratio, kernel_size = kernel_cbam)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.num_batches_tracked += 1\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        if self.use_cbam:\n",
        "            out = self.cbam(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        if self.drop_rate > 0:\n",
        "            out = F.dropout(out, p=self.drop_rate, training=self.training, inplace=True)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block=BasicBlock, keep_prob=1.0, avg_pool=False, drop_rate=0.0, dropblock_size=5, reduction_ratio = 1, kernel_cbam = 3, use_cbam_block= True, use_cbam_class = True):\n",
        "        self.inplanes = 3\n",
        "        super(ResNet, self).__init__()\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.kernel_cbam = kernel_cbam\n",
        "        self.use_cbam_block = use_cbam_block\n",
        "        self.use_cbam_class = use_cbam_class\n",
        "        print(use_cbam_block, use_cbam_class)\n",
        "\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, stride=2, drop_rate=drop_rate)\n",
        "        self.layer2 = self._make_layer(block, 160, stride=2, drop_rate=drop_rate)\n",
        "        self.layer3 = self._make_layer(block, 320, stride=2, drop_rate=drop_rate, drop_block=True,\n",
        "                                       block_size=dropblock_size)\n",
        "        self.layer4 = self._make_layer(block, 640, stride=2, drop_rate=drop_rate, drop_block=True,\n",
        "                                       block_size=dropblock_size)\n",
        "        if self.use_cbam_class:\n",
        "            self.cbam = CBAM(n_channels_in = 640*block.expansion, reduction_ratio = reduction_ratio, kernel_size = kernel_cbam)\n",
        "\n",
        "        if avg_pool:\n",
        "            self.avgpool = nn.AvgPool2d(5, stride=1)\n",
        "        self.keep_prob = keep_prob\n",
        "        self.keep_avg_pool = avg_pool\n",
        "        self.dropout = nn.Dropout(p=1 - self.keep_prob, inplace=False)\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "        for m in self.modules():\n",
        "          if isinstance(m,nn.Conv2d):\n",
        "              nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "          elif isinstance(m, nn.BatchNorm2d):\n",
        "              nn.init.constant_(m.weight, 1)\n",
        "              nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, stride=1, drop_rate=0.0, drop_block=False, block_size=1):\n",
        "        downsample = None \n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, drop_rate, drop_block, block_size))\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        if self.use_cbam_class:\n",
        "            x = x  + self.cbam(x)\n",
        "\n",
        "        #return x\n",
        "        return x.view(x.size(0),-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing resnet12.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xy6uXEAGBhW"
      },
      "source": [
        "!mv -f resnet12.py ./gdrive/My\\ Drive/IITB\\ Internship/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVx3TGQJd88t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcdabb2-24c6-4913-aa2e-74c2f437daba"
      },
      "source": [
        "%%writefile cub_ds.py\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import os.path as osp\n",
        "import pickle\n",
        "from PIL import Image\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "class CUB2002011_ds(Dataset):\n",
        "    def __init__(self, args, mode=\"train\"):\n",
        "        assert mode in [\"train\", \"test\"] #Make sure the mode is one of the two\n",
        "        super(CUB2002011_ds, self).__init__() #Inheritance dataset __init__()\n",
        "        train_test_split = pd.read_csv(\n",
        "            f\"{args.data_dir}/train_test_split.txt\",\n",
        "            sep=\" \",\n",
        "            names=[\"img_id\", \"is_train\"],\n",
        "        )\n",
        "        img_paths = pd.read_csv(\n",
        "            f\"{args.data_dir}/images.txt\", sep=\" \", names=[\"img_id\", \"img_path\"]\n",
        "        )  #Read all image paths\n",
        "        labels = pd.read_csv(\n",
        "            f\"{args.data_dir}/image_class_labels.txt\",\n",
        "            sep=\" \",\n",
        "            names=[\"img_id\", \"img_label\"],\n",
        "        )\n",
        "        labels_after_noise = pd.read_csv(\n",
        "            f\"{args.data_dir}/Y_After_Noise.txt\",\n",
        "            # sep=\" \",\n",
        "            names=[\"img_label\"],\n",
        "        )\n",
        "        \n",
        "        self.data_dir = osp.join(args.data_dir, \"images\")\n",
        "        self.mode = 1 if mode == \"train\" else 0  #Choose whether it is 1 training mode or test mode 0\n",
        "\n",
        "\n",
        "        self.img_path = img_paths.loc[\n",
        "            train_test_split[\"is_train\"] == self.mode, \"img_path\"\n",
        "        ].values  #Get the corresponding path in the corresponding mode\n",
        "        self.label = labels.loc[\n",
        "            train_test_split[\"is_train\"] == self.mode, \"img_label\"\n",
        "        ].values #Get the label in the corresponding mode\n",
        "        # if self.mode:\n",
        "        #   labels[\"img_label\"] = labels_after_noise[\"img_label\"]\n",
        "        if self.mode:\n",
        "          self.label = labels_after_noise[\"img_label\"].values#.astype(int)\n",
        "        img_sz = 84\n",
        "        if mode == 'test':\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((img_sz, img_sz)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((img_sz, img_sz)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(30),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "        self.y = self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.label.shape[0] #The size of the entire data set in the corresponding mode\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.transform(\n",
        "            Image.open(f\"{self.data_dir}/{self.img_path[idx]}\").convert(\"RGB\")\n",
        "        ) #Open the path of the image in the corresponding folder of the data set and convert the image data to RGB\n",
        "        # label = int(self.label[idx])\n",
        "        label = self.label[idx]\n",
        "        return img, label-1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cub_ds.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWrtq3DLGvJC"
      },
      "source": [
        "!mv -f cub_ds.py ./gdrive/My\\ Drive/IITB\\ Internship/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DVh-WX4eE6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f0be3c-495d-467d-dad4-a8294fde83ca"
      },
      "source": [
        "%%writefile train.py \n",
        "# coding=utf-8\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/IITB Internship/src/Prototypical-Networks-for-Few-shot-Learning-PyTorch/src/')\n",
        "\n",
        "from prototypical_batch_sampler import PrototypicalBatchSampler\n",
        "from prototypical_loss import prototypical_loss as loss_fn\n",
        "from omniglot_dataset import OmniglotDataset\n",
        "from cub_ds import *\n",
        "from protonet import ProtoNet\n",
        "from resnet12 import *\n",
        "from parser_util import get_parser\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "def init_seed(opt):\n",
        "    '''\n",
        "    Disable cudnn to maximize reproducibility\n",
        "    '''\n",
        "    torch.cuda.cudnn_enabled = False\n",
        "    np.random.seed(opt.manual_seed)\n",
        "    torch.manual_seed(opt.manual_seed)\n",
        "    torch.cuda.manual_seed(opt.manual_seed)\n",
        "\n",
        "\n",
        "def init_dataset(opt, mode):\n",
        "    mode = 'test' if mode == 'test' else 'train'\n",
        "    parser=argparse.ArgumentParser()\n",
        "    parser.add_argument('-data_dir',default='')\n",
        "    args=parser.parse_args([])\n",
        "    \n",
        "    dataset = CUB2002011_ds(args,mode)#OmniglotDataset(mode=mode, root=opt.dataset_root)\n",
        "    n_classes = len(np.unique(dataset.y))\n",
        "    if n_classes < opt.classes_per_it_tr or n_classes < opt.classes_per_it_val:\n",
        "        raise(Exception('There are not enough classes in the dataset in order ' +\n",
        "                        'to satisfy the chosen classes_per_it. Decrease the ' +\n",
        "                        'classes_per_it_{tr/val} option and try again.'))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def init_sampler(opt, labels, mode):\n",
        "    if 'train' in mode:\n",
        "        classes_per_it = opt.classes_per_it_tr\n",
        "        num_samples = opt.num_support_tr + opt.num_query_tr\n",
        "    else:\n",
        "        classes_per_it = opt.classes_per_it_val\n",
        "        num_samples = opt.num_support_val + opt.num_query_val\n",
        "\n",
        "    return PrototypicalBatchSampler(labels=labels,\n",
        "                                    classes_per_it=classes_per_it,\n",
        "                                    num_samples=num_samples,\n",
        "                                    iterations=opt.iterations)\n",
        "\n",
        "\n",
        "def init_dataloader(opt, mode):\n",
        "    mode = 'test' if mode == 'test' else 'train'\n",
        "    parser=argparse.ArgumentParser()\n",
        "    parser.add_argument('-data_dir',default='gdrive/My Drive/IITB Internship/CUB_200_2011')\n",
        "    args=parser.parse_args([])\n",
        "    \n",
        "    dataset = CUB2002011_ds(args,mode)\n",
        "    #dataset = init_dataset(opt, mode)\n",
        "    sampler = init_sampler(opt, dataset.y, mode)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def init_protonet(opt, mode='res12'):\n",
        "    '''\n",
        "    Initialize the ProtoNet\n",
        "    '''\n",
        "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
        "    if mode == '4conv':\n",
        "        model = ProtoNet().to(device)\n",
        "    else:\n",
        "        # resnet == resnet12\n",
        "        model = ResNet().to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "def init_optim(opt, model):\n",
        "    '''\n",
        "    Initialize optimizer\n",
        "    '''\n",
        "    return torch.optim.Adam(params=model.parameters(),\n",
        "                            lr=opt.learning_rate)\n",
        "\n",
        "\n",
        "def init_lr_scheduler(opt, optim, mode='onecycle'):\n",
        "    '''\n",
        "    Initialize the learning rate scheduler\n",
        "    '''\n",
        "    if mode == 'step':\n",
        "        return torch.optim.lr_scheduler.StepLR(optimizer=optim,\n",
        "                                               gamma=opt.lr_scheduler_gamma,\n",
        "                                               step_size=opt.lr_scheduler_step)\n",
        "    else:\n",
        "        return torch.optim.lr_scheduler.OneCycleLR(optim,\n",
        "                                                   total_steps = opt.epochs,\n",
        "                                                   max_lr = opt.learning_rate)\n",
        "\n",
        "\n",
        "def save_list_to_file(path, thelist):\n",
        "    with open(path, 'w') as f:\n",
        "        for item in thelist:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "def train(opt, tr_dataloader, model, optim, lr_scheduler, val_dataloader=None):\n",
        "    '''\n",
        "    Train the model with the prototypical learning algorithm\n",
        "    '''\n",
        "\n",
        "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
        "\n",
        "    if val_dataloader is None:\n",
        "        best_state = None\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "    best_acc = 0\n",
        "\n",
        "    best_model_path = os.path.join(opt.experiment_root, 'best_model.pth')\n",
        "    last_model_path = os.path.join(opt.experiment_root, 'last_model.pth')\n",
        "\n",
        "    for epoch in range(opt.epochs): #opt.epochs\n",
        "        print('=== Epoch: {} ==='.format(epoch))\n",
        "        tr_iter = iter(tr_dataloader)\n",
        "        model.train()\n",
        "        for batch in tqdm(tr_iter):\n",
        "            optim.zero_grad()\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            model_output = model(x)\n",
        "            loss, acc = loss_fn(model_output, target=y,\n",
        "                                n_support=opt.num_support_tr)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            train_loss.append(loss.item())\n",
        "            train_acc.append(acc.item())\n",
        "        avg_loss = np.mean(train_loss[-opt.iterations:])\n",
        "        avg_acc = np.mean(train_acc[-opt.iterations:])\n",
        "        print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_loss, avg_acc))\n",
        "        lr_scheduler.step()\n",
        "        '''if val_dataloader is None:\n",
        "            continue\n",
        "        val_iter = iter(val_dataloader)\n",
        "        model.eval()\n",
        "        for batch in val_iter:\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            model_output = model(x)\n",
        "            loss, acc = loss_fn(model_output, target=y,\n",
        "                                n_support=opt.num_support_val)\n",
        "            val_loss.append(loss.item())\n",
        "            val_acc.append(acc.item())\n",
        "        avg_loss = np.mean(val_loss[-opt.iterations:])\n",
        "        avg_acc = np.mean(val_acc[-opt.iterations:])\n",
        "        postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(\n",
        "            best_acc)\n",
        "        print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(\n",
        "            avg_loss, avg_acc, postfix))\n",
        "        if avg_acc >= best_acc:\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            best_acc = avg_acc\n",
        "            best_state = model.state_dict()'''\n",
        "\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "\n",
        "    for name in ['train_loss', 'train_acc', 'val_loss', 'val_acc']:\n",
        "        save_list_to_file(os.path.join(opt.experiment_root,\n",
        "                                       name + '.txt'), locals()[name])\n",
        "\n",
        "    #return best_state, \n",
        "    return best_acc, train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "\n",
        "def test(opt, test_dataloader, model):\n",
        "    '''\n",
        "    Test the model trained with the prototypical learning algorithm\n",
        "    '''\n",
        "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
        "    avg_acc = list()\n",
        "    model.eval()\n",
        "    for epoch in range(10):\n",
        "        test_iter = iter(test_dataloader)\n",
        "        for batch in test_iter:\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                model_output = model(x)\n",
        "            _, acc = loss_fn(model_output, target=y,\n",
        "                             n_support=opt.num_support_val)\n",
        "            avg_acc.append(acc.item())\n",
        "    avg_acc = np.mean(avg_acc)\n",
        "    print('Test Acc: {}'.format(avg_acc))\n",
        "\n",
        "    return avg_acc\n",
        "\n",
        "\n",
        "def eval(opt):\n",
        "    '''\n",
        "    Initialize everything and train\n",
        "    '''\n",
        "    options = get_parser().parse_args()\n",
        "\n",
        "    if torch.cuda.is_available() and not options.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "    #init_seed(options)\n",
        "    test_dataloader = init_dataset(options)[-1]\n",
        "    model = init_protonet(options)\n",
        "    model_path = os.path.join(opt.experiment_root, 'best_model.pth')\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    test(opt=options,\n",
        "         test_dataloader=test_dataloader,\n",
        "         model=model)\n",
        "\n",
        "\n",
        "def main():\n",
        "    '''\n",
        "    Initialize everything and train\n",
        "    '''\n",
        "    options = get_parser().parse_args()\n",
        "    \n",
        "    options.epochs = 200\n",
        "    options.iterations = 100\n",
        "    \n",
        "    options.classes_per_it_tr = 5\n",
        "    options.num_support_tr = 5\n",
        "    options.num_query_tr = 5\n",
        "    \n",
        "    options.classes_per_it_val = 5\n",
        "    options.num_support_val = 5\n",
        "    options.num_query_val = 5\n",
        "    \n",
        "    #options.learing_rate = 3e-04\n",
        "    \n",
        "    if not os.path.exists(options.experiment_root):\n",
        "        os.makedirs(options.experiment_root)\n",
        "\n",
        "    if torch.cuda.is_available() and not options.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "    #init_seed(options)\n",
        "\n",
        "    tr_dataloader = init_dataloader(options, 'train')\n",
        "    val_dataloader = init_dataloader(options, 'val')\n",
        "    # trainval_dataloader = init_dataloader(options, 'trainval')\n",
        "    test_dataloader = init_dataloader(options, 'test')\n",
        "\n",
        "    model = init_protonet(options, mode='res12')\n",
        "    optim = init_optim(options, model)\n",
        "    lr_scheduler = init_lr_scheduler(options, optim, mode='onecycle')\n",
        "    res = train(opt=options,\n",
        "                tr_dataloader=tr_dataloader,\n",
        "                val_dataloader=val_dataloader,\n",
        "                model=model,\n",
        "                optim=optim,\n",
        "                lr_scheduler=lr_scheduler)\n",
        "    #best_state, \n",
        "    best_acc, train_loss, train_acc, val_loss, val_acc = res\n",
        "    print('Testing with last model..')\n",
        "    test(opt=options,\n",
        "         test_dataloader=test_dataloader,\n",
        "         model=model)\n",
        "\n",
        "    '''model.load_state_dict(best_state)\n",
        "    print('Testing with best model..')\n",
        "    test(opt=options,\n",
        "         test_dataloader=test_dataloader,\n",
        "         model=model)'''\n",
        "\n",
        "    # optim = init_optim(options, model)\n",
        "    # lr_scheduler = init_lr_scheduler(options, optim)\n",
        "\n",
        "    # print('Training on train+val set..')\n",
        "    # train(opt=options,\n",
        "    #       tr_dataloader=trainval_dataloader,\n",
        "    #       val_dataloader=None,\n",
        "    #       model=model,\n",
        "    #       optim=optim,\n",
        "    #       lr_scheduler=lr_scheduler)\n",
        "\n",
        "    # print('Testing final model..')\n",
        "    # test(opt=options,\n",
        "    #      test_dataloader=test_dataloader,\n",
        "    #      model=model)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYAfeNd7erT1"
      },
      "source": [
        "!mv -f train.py ./gdrive/My\\ Drive/IITB\\ Internship/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 \"./gdrive/My Drive/IITB Internship/src/train.py\" --cuda"
      ],
      "metadata": {
        "id": "EiaJrh0VOnhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1edf84-a0d1-4cf0-c23a-68d3b09b479e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True True\n",
            "=== Epoch: 0 ===\n",
            "100% 100/100 [15:21<00:00,  9.22s/it]\n",
            "Avg Train Loss: 651.6306309509278, Avg Train Acc: 0.42039999797940253\n",
            "=== Epoch: 1 ===\n",
            "100% 100/100 [07:29<00:00,  4.50s/it]\n",
            "Avg Train Loss: 285.80106483459474, Avg Train Acc: 0.450399998575449\n",
            "=== Epoch: 2 ===\n",
            "100% 100/100 [03:58<00:00,  2.38s/it]\n",
            "Avg Train Loss: 187.91091384887696, Avg Train Acc: 0.4663999989628792\n",
            "=== Epoch: 3 ===\n",
            "100% 100/100 [02:25<00:00,  1.45s/it]\n",
            "Avg Train Loss: 155.05956043243407, Avg Train Acc: 0.46239999987185\n",
            "=== Epoch: 4 ===\n",
            "100% 100/100 [02:01<00:00,  1.22s/it]\n",
            "Avg Train Loss: 124.02802410125733, Avg Train Acc: 0.47759999811649323\n",
            "=== Epoch: 5 ===\n",
            "100% 100/100 [01:44<00:00,  1.05s/it]\n",
            "Avg Train Loss: 114.4257451248169, Avg Train Acc: 0.4675999999046326\n",
            "=== Epoch: 6 ===\n",
            "100% 100/100 [01:33<00:00,  1.07it/s]\n",
            "Avg Train Loss: 96.20867517471314, Avg Train Acc: 0.4919999983906746\n",
            "=== Epoch: 7 ===\n",
            "100% 100/100 [01:31<00:00,  1.10it/s]\n",
            "Avg Train Loss: 92.93299333572388, Avg Train Acc: 0.4671999990940094\n",
            "=== Epoch: 8 ===\n",
            "100% 100/100 [01:27<00:00,  1.14it/s]\n",
            "Avg Train Loss: 75.29314870834351, Avg Train Acc: 0.5027999997138977\n",
            "=== Epoch: 9 ===\n",
            "100% 100/100 [01:28<00:00,  1.14it/s]\n",
            "Avg Train Loss: 74.73401247978211, Avg Train Acc: 0.4887999984622002\n",
            "=== Epoch: 10 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 67.88452806472779, Avg Train Acc: 0.48000000074505805\n",
            "=== Epoch: 11 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 58.65882785797119, Avg Train Acc: 0.4971999979019165\n",
            "=== Epoch: 12 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 56.28072301864624, Avg Train Acc: 0.487200001180172\n",
            "=== Epoch: 13 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 45.64914472579956, Avg Train Acc: 0.4903999990224838\n",
            "=== Epoch: 14 ===\n",
            "100% 100/100 [01:27<00:00,  1.15it/s]\n",
            "Avg Train Loss: 41.3651570892334, Avg Train Acc: 0.4971999990940094\n",
            "=== Epoch: 15 ===\n",
            "100% 100/100 [01:27<00:00,  1.14it/s]\n",
            "Avg Train Loss: 35.20108279228211, Avg Train Acc: 0.5019999997317791\n",
            "=== Epoch: 16 ===\n",
            "100% 100/100 [01:27<00:00,  1.14it/s]\n",
            "Avg Train Loss: 27.458357648849486, Avg Train Acc: 0.5284000000357628\n",
            "=== Epoch: 17 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 23.588196773529052, Avg Train Acc: 0.5011999973654747\n",
            "=== Epoch: 18 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 19.5349312543869, Avg Train Acc: 0.49440000012516977\n",
            "=== Epoch: 19 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 15.093395109176635, Avg Train Acc: 0.49199999921023846\n",
            "=== Epoch: 20 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 12.852553992271423, Avg Train Acc: 0.48879999950528147\n",
            "=== Epoch: 21 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 9.3769091796875, Avg Train Acc: 0.4891999998688698\n",
            "=== Epoch: 22 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 7.087489042282105, Avg Train Acc: 0.5008000026643277\n",
            "=== Epoch: 23 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 5.461195262670517, Avg Train Acc: 0.5240000002086163\n",
            "=== Epoch: 24 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 4.628998568058014, Avg Train Acc: 0.5128000004589558\n",
            "=== Epoch: 25 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 3.96046919465065, Avg Train Acc: 0.49920000061392783\n",
            "=== Epoch: 26 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 3.1279747700691223, Avg Train Acc: 0.5208000004291534\n",
            "=== Epoch: 27 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 2.28194947719574, Avg Train Acc: 0.5504000018537044\n",
            "=== Epoch: 28 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 2.168976711034775, Avg Train Acc: 0.5276000000536442\n",
            "=== Epoch: 29 ===\n",
            "100% 100/100 [01:27<00:00,  1.14it/s]\n",
            "Avg Train Loss: 1.8551921850442887, Avg Train Acc: 0.537200001180172\n",
            "=== Epoch: 30 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.6300476351380349, Avg Train Acc: 0.5288000002503395\n",
            "=== Epoch: 31 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.5330137246847153, Avg Train Acc: 0.5348000015318394\n",
            "=== Epoch: 32 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.365325214266777, Avg Train Acc: 0.5383999998867511\n",
            "=== Epoch: 33 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.2833284935355187, Avg Train Acc: 0.5563999992609024\n",
            "=== Epoch: 34 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.210579832494259, Avg Train Acc: 0.5632000008225441\n",
            "=== Epoch: 35 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.1660068026185035, Avg Train Acc: 0.5755999983847141\n",
            "=== Epoch: 36 ===\n",
            "100% 100/100 [01:27<00:00,  1.14it/s]\n",
            "Avg Train Loss: 1.128698162138462, Avg Train Acc: 0.5648000025749207\n",
            "=== Epoch: 37 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.1403154170513152, Avg Train Acc: 0.579200000166893\n",
            "=== Epoch: 38 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.1116271835565568, Avg Train Acc: 0.5940000019967556\n",
            "=== Epoch: 39 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.081691865324974, Avg Train Acc: 0.5927999991178513\n",
            "=== Epoch: 40 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.0328415939211846, Avg Train Acc: 0.6180000026524067\n",
            "=== Epoch: 41 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.0674871611595154, Avg Train Acc: 0.6004000023007393\n",
            "=== Epoch: 42 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.005183675289154, Avg Train Acc: 0.6092000013589859\n",
            "=== Epoch: 43 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.0277458381652833, Avg Train Acc: 0.5988000041246414\n",
            "=== Epoch: 44 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9632236272096634, Avg Train Acc: 0.6208000040054321\n",
            "=== Epoch: 45 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9859080991148949, Avg Train Acc: 0.6107999986410141\n",
            "=== Epoch: 46 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.0601750552654265, Avg Train Acc: 0.604400002360344\n",
            "=== Epoch: 47 ===\n",
            "100% 100/100 [01:27<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.924175586104393, Avg Train Acc: 0.6256000013649463\n",
            "=== Epoch: 48 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.9263799673318863, Avg Train Acc: 0.6376000007987023\n",
            "=== Epoch: 49 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9105586168169976, Avg Train Acc: 0.6292000029981136\n",
            "=== Epoch: 50 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.9622349137067795, Avg Train Acc: 0.6203999987244606\n",
            "=== Epoch: 51 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9251989760994911, Avg Train Acc: 0.6404000020027161\n",
            "=== Epoch: 52 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.9161412584781646, Avg Train Acc: 0.6392000022530556\n",
            "=== Epoch: 53 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.9253983613848686, Avg Train Acc: 0.6451999998092651\n",
            "=== Epoch: 54 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.9386668738722801, Avg Train Acc: 0.64280000269413\n",
            "=== Epoch: 55 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8290006625652313, Avg Train Acc: 0.6764000034332276\n",
            "=== Epoch: 56 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.8779467576742173, Avg Train Acc: 0.6600000050663948\n",
            "=== Epoch: 57 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9590167951583862, Avg Train Acc: 0.6416000027954578\n",
            "=== Epoch: 58 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.8485673299431801, Avg Train Acc: 0.6652000030875206\n",
            "=== Epoch: 59 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8475636449456215, Avg Train Acc: 0.6760000032186508\n",
            "=== Epoch: 60 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.8030739489197731, Avg Train Acc: 0.6840000006556511\n",
            "=== Epoch: 61 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8228093899786473, Avg Train Acc: 0.6620000050961972\n",
            "=== Epoch: 62 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8229581898450852, Avg Train Acc: 0.6755999988317489\n",
            "=== Epoch: 63 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8656377869844437, Avg Train Acc: 0.6540000039339066\n",
            "=== Epoch: 64 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8029925578832626, Avg Train Acc: 0.6844000011682511\n",
            "=== Epoch: 65 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.7935699144005776, Avg Train Acc: 0.688000001013279\n",
            "=== Epoch: 66 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8187226492166519, Avg Train Acc: 0.6720000022649765\n",
            "=== Epoch: 67 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8216005936264992, Avg Train Acc: 0.6772000020742417\n",
            "=== Epoch: 68 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.7665471805632115, Avg Train Acc: 0.7032000035047531\n",
            "=== Epoch: 69 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8073147058486938, Avg Train Acc: 0.6868000039458275\n",
            "=== Epoch: 70 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 15.212274365127087, Avg Train Acc: 0.5020000010728836\n",
            "=== Epoch: 71 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.4297682243585585, Avg Train Acc: 0.4388000011444092\n",
            "=== Epoch: 72 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.3234319007396698, Avg Train Acc: 0.4548000003397465\n",
            "=== Epoch: 73 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.229673700928688, Avg Train Acc: 0.5071999979019165\n",
            "=== Epoch: 74 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.1772975128889085, Avg Train Acc: 0.524000001102686\n",
            "=== Epoch: 75 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.1482822388410567, Avg Train Acc: 0.5343999999761582\n",
            "=== Epoch: 76 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.0964571660757065, Avg Train Acc: 0.5680000013113022\n",
            "=== Epoch: 77 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 1.0919952234625816, Avg Train Acc: 0.5647999995946884\n",
            "=== Epoch: 78 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.0649420228600501, Avg Train Acc: 0.5784000004827976\n",
            "=== Epoch: 79 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 1.0443063047528267, Avg Train Acc: 0.6063999983668328\n",
            "=== Epoch: 80 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.006991426050663, Avg Train Acc: 0.5936000004410744\n",
            "=== Epoch: 81 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9918916928768158, Avg Train Acc: 0.6056000027060509\n",
            "=== Epoch: 82 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 1.0012834817171097, Avg Train Acc: 0.6092000013589859\n",
            "=== Epoch: 83 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.9724279969930649, Avg Train Acc: 0.6168000018596649\n",
            "=== Epoch: 84 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9741000723838806, Avg Train Acc: 0.6196000015735627\n",
            "=== Epoch: 85 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.988709799349308, Avg Train Acc: 0.6104000005125999\n",
            "=== Epoch: 86 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9730056369304657, Avg Train Acc: 0.6179999995231629\n",
            "=== Epoch: 87 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9367319011688232, Avg Train Acc: 0.6364000022411347\n",
            "=== Epoch: 88 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.9161394181847572, Avg Train Acc: 0.6344000020623207\n",
            "=== Epoch: 89 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8416848734021187, Avg Train Acc: 0.670800002515316\n",
            "=== Epoch: 90 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8718606376647949, Avg Train Acc: 0.658800005018711\n",
            "=== Epoch: 91 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8940735119581222, Avg Train Acc: 0.638800003528595\n",
            "=== Epoch: 92 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.8623645767569542, Avg Train Acc: 0.65200000166893\n",
            "=== Epoch: 93 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.848467399775982, Avg Train Acc: 0.6632000023126602\n",
            "=== Epoch: 94 ===\n",
            "100% 100/100 [01:27<00:00,  1.14it/s]\n",
            "Avg Train Loss: 0.8319223943352699, Avg Train Acc: 0.6584000027179718\n",
            "=== Epoch: 95 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8955030310153961, Avg Train Acc: 0.6448000058531761\n",
            "=== Epoch: 96 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.8596650183200836, Avg Train Acc: 0.6516000026464462\n",
            "=== Epoch: 97 ===\n",
            "100% 100/100 [01:27<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.8295545202493667, Avg Train Acc: 0.6644000002741813\n",
            "=== Epoch: 98 ===\n",
            "100% 100/100 [01:27<00:00,  1.14it/s]\n",
            "Avg Train Loss: 0.8277028393745423, Avg Train Acc: 0.6644000047445298\n",
            "=== Epoch: 99 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.816154060959816, Avg Train Acc: 0.6736000014841557\n",
            "=== Epoch: 100 ===\n",
            "100% 100/100 [01:25<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.833728879392147, Avg Train Acc: 0.6640000033378601\n",
            "=== Epoch: 101 ===\n",
            "100% 100/100 [01:27<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.8191558650135994, Avg Train Acc: 0.6664000022411346\n",
            "=== Epoch: 102 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.8095917096734047, Avg Train Acc: 0.6788000020384789\n",
            "=== Epoch: 103 ===\n",
            "100% 100/100 [01:26<00:00,  1.15it/s]\n",
            "Avg Train Loss: 0.7871392130851745, Avg Train Acc: 0.6824000027775764\n",
            "=== Epoch: 104 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.801539017856121, Avg Train Acc: 0.679600002169609\n",
            "=== Epoch: 105 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7910970066487789, Avg Train Acc: 0.6968000036478043\n",
            "=== Epoch: 106 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.8018549604713917, Avg Train Acc: 0.6819999998807907\n",
            "=== Epoch: 107 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7900135347247124, Avg Train Acc: 0.686000000089407\n",
            "=== Epoch: 108 ===\n",
            "100% 100/100 [01:25<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.7496978026628495, Avg Train Acc: 0.7024000039696694\n",
            "=== Epoch: 109 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7284193140268326, Avg Train Acc: 0.7096000024676323\n",
            "=== Epoch: 110 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7775825774669647, Avg Train Acc: 0.6964000013470649\n",
            "=== Epoch: 111 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7748988324403763, Avg Train Acc: 0.6900000032782555\n",
            "=== Epoch: 112 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7368190556764602, Avg Train Acc: 0.7072000056505203\n",
            "=== Epoch: 113 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7681962651014328, Avg Train Acc: 0.6892000025510788\n",
            "=== Epoch: 114 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7397681874036789, Avg Train Acc: 0.7100000017881394\n",
            "=== Epoch: 115 ===\n",
            "100% 100/100 [01:26<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.7139172257483005, Avg Train Acc: 0.7172000017762185\n",
            "=== Epoch: 116 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6883431358635426, Avg Train Acc: 0.7268000021576881\n",
            "=== Epoch: 117 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7089500763267279, Avg Train Acc: 0.7088000014424324\n",
            "=== Epoch: 118 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7460893189907074, Avg Train Acc: 0.7020000031590462\n",
            "=== Epoch: 119 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7426321496069431, Avg Train Acc: 0.7200000029802323\n",
            "=== Epoch: 120 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.706424314826727, Avg Train Acc: 0.7243999981880188\n",
            "=== Epoch: 121 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6887018324434757, Avg Train Acc: 0.7303999999165535\n",
            "=== Epoch: 122 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6494393250346184, Avg Train Acc: 0.7420000022649765\n",
            "=== Epoch: 123 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7131993836164474, Avg Train Acc: 0.7259999996423722\n",
            "=== Epoch: 124 ===\n",
            "100% 100/100 [01:25<00:00,  1.16it/s]\n",
            "Avg Train Loss: 0.6784428282082081, Avg Train Acc: 0.7336000043153763\n",
            "=== Epoch: 125 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.7043268902599812, Avg Train Acc: 0.7220000046491623\n",
            "=== Epoch: 126 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6686764611303806, Avg Train Acc: 0.7388000020384788\n",
            "=== Epoch: 127 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6946758475899696, Avg Train Acc: 0.7275999990105629\n",
            "=== Epoch: 128 ===\n",
            "100% 100/100 [01:25<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.6031100253760815, Avg Train Acc: 0.7664000019431114\n",
            "=== Epoch: 129 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.71158789858222, Avg Train Acc: 0.7236000004410744\n",
            "=== Epoch: 130 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6667496229708195, Avg Train Acc: 0.7455999997258186\n",
            "=== Epoch: 131 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6722327099740505, Avg Train Acc: 0.7384000021219254\n",
            "=== Epoch: 132 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6621433670818806, Avg Train Acc: 0.731600002348423\n",
            "=== Epoch: 133 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6562681068480015, Avg Train Acc: 0.735600004196167\n",
            "=== Epoch: 134 ===\n",
            "100% 100/100 [01:24<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.616403146982193, Avg Train Acc: 0.7656000006198883\n",
            "=== Epoch: 135 ===\n",
            "100% 100/100 [01:24<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.642867446988821, Avg Train Acc: 0.7444000029563904\n",
            "=== Epoch: 136 ===\n",
            "100% 100/100 [01:24<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.6282120066881179, Avg Train Acc: 0.7640000027418137\n",
            "=== Epoch: 137 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.5918971547484397, Avg Train Acc: 0.778400001525879\n",
            "=== Epoch: 138 ===\n",
            "100% 100/100 [01:25<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.575164520740509, Avg Train Acc: 0.7828000012040138\n",
            "=== Epoch: 139 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.5850434759259224, Avg Train Acc: 0.7671999987959862\n",
            "=== Epoch: 140 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6064176829904318, Avg Train Acc: 0.7612000033259392\n",
            "=== Epoch: 141 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.62276516251266, Avg Train Acc: 0.7567999990284443\n",
            "=== Epoch: 142 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6013506029546261, Avg Train Acc: 0.7468000024557113\n",
            "=== Epoch: 143 ===\n",
            "100% 100/100 [01:24<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.6133563882112503, Avg Train Acc: 0.7628000000119209\n",
            "=== Epoch: 144 ===\n",
            "100% 100/100 [01:25<00:00,  1.17it/s]\n",
            "Avg Train Loss: 0.6059647666662932, Avg Train Acc: 0.7720000010728836\n",
            "=== Epoch: 145 ===\n",
            "100% 100/100 [01:24<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.5831198170036077, Avg Train Acc: 0.7704000005125999\n",
            "=== Epoch: 146 ===\n",
            "100% 100/100 [01:24<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.5896296998113394, Avg Train Acc: 0.7811999997496605\n",
            "=== Epoch: 147 ===\n",
            "100% 100/100 [01:24<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.5546696128323674, Avg Train Acc: 0.7788000029325485\n",
            "=== Epoch: 148 ===\n",
            "100% 100/100 [01:24<00:00,  1.18it/s]\n",
            "Avg Train Loss: 0.5700437051057815, Avg Train Acc: 0.7647999989986419\n",
            "=== Epoch: 149 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.6099371162801981, Avg Train Acc: 0.7636000010371208\n",
            "=== Epoch: 150 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.5772219717502594, Avg Train Acc: 0.7848000010848045\n",
            "=== Epoch: 151 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.5319330651313067, Avg Train Acc: 0.7959999972581864\n",
            "=== Epoch: 152 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.5889427722245455, Avg Train Acc: 0.7783999988436698\n",
            "=== Epoch: 153 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.5740974122285842, Avg Train Acc: 0.7691999992728233\n",
            "=== Epoch: 154 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.5462918994575738, Avg Train Acc: 0.7811999997496605\n",
            "=== Epoch: 155 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.5778708000481129, Avg Train Acc: 0.769600000679493\n",
            "=== Epoch: 156 ===\n",
            "100% 100/100 [01:23<00:00,  1.19it/s]\n",
            "Avg Train Loss: 0.5274048548936844, Avg Train Acc: 0.8008000007271767\n",
            "=== Epoch: 157 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.5508720622956753, Avg Train Acc: 0.7916000005602837\n",
            "=== Epoch: 158 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.5682624824345112, Avg Train Acc: 0.7744000029563903\n",
            "=== Epoch: 159 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.54035230319947, Avg Train Acc: 0.786400001347065\n",
            "=== Epoch: 160 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.5156629402190447, Avg Train Acc: 0.8035999995470047\n",
            "=== Epoch: 161 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.48339186325669287, Avg Train Acc: 0.8128000012040139\n",
            "=== Epoch: 162 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.49694640703499315, Avg Train Acc: 0.8103999987244606\n",
            "=== Epoch: 163 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.49620843037962914, Avg Train Acc: 0.8023999997973442\n",
            "=== Epoch: 164 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.4545074200630188, Avg Train Acc: 0.8203999972343445\n",
            "=== Epoch: 165 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.5133274488523603, Avg Train Acc: 0.8003999984264374\n",
            "=== Epoch: 166 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.502698435485363, Avg Train Acc: 0.8052000033855439\n",
            "=== Epoch: 167 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.50163429915905, Avg Train Acc: 0.8047999984025955\n",
            "=== Epoch: 168 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.46654070548713206, Avg Train Acc: 0.820799998641014\n",
            "=== Epoch: 169 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.5217166850529611, Avg Train Acc: 0.793199999332428\n",
            "=== Epoch: 170 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.4881089889258146, Avg Train Acc: 0.8055999991297722\n",
            "=== Epoch: 171 ===\n",
            "100% 100/100 [01:22<00:00,  1.22it/s]\n",
            "Avg Train Loss: 0.4858848845213652, Avg Train Acc: 0.8007999995350837\n",
            "=== Epoch: 172 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.45178097881376744, Avg Train Acc: 0.8231999993324279\n",
            "=== Epoch: 173 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.48735539749264717, Avg Train Acc: 0.8024000003933907\n",
            "=== Epoch: 174 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.5062132521718741, Avg Train Acc: 0.801600000411272\n",
            "=== Epoch: 175 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.48634871348738673, Avg Train Acc: 0.8068000006675721\n",
            "=== Epoch: 176 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.4254946985654533, Avg Train Acc: 0.8363999962806702\n",
            "=== Epoch: 177 ===\n",
            "100% 100/100 [01:22<00:00,  1.22it/s]\n",
            "Avg Train Loss: 0.4924004689604044, Avg Train Acc: 0.8151999998092652\n",
            "=== Epoch: 178 ===\n",
            "100% 100/100 [01:22<00:00,  1.22it/s]\n",
            "Avg Train Loss: 0.451947309859097, Avg Train Acc: 0.8167999976873398\n",
            "=== Epoch: 179 ===\n",
            "100% 100/100 [01:21<00:00,  1.23it/s]\n",
            "Avg Train Loss: 0.4245227071642876, Avg Train Acc: 0.8331999969482422\n",
            "=== Epoch: 180 ===\n",
            "100% 100/100 [01:21<00:00,  1.22it/s]\n",
            "Avg Train Loss: 0.46847139298915863, Avg Train Acc: 0.8156000006198884\n",
            "=== Epoch: 181 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.47254340916872023, Avg Train Acc: 0.8068000006675721\n",
            "=== Epoch: 182 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.43086030378937723, Avg Train Acc: 0.8315999978780746\n",
            "=== Epoch: 183 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.43452662009745835, Avg Train Acc: 0.825999998152256\n",
            "=== Epoch: 184 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.4536759200692177, Avg Train Acc: 0.823599997162819\n",
            "=== Epoch: 185 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.43034572325646875, Avg Train Acc: 0.8319999969005585\n",
            "=== Epoch: 186 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.3936968132853508, Avg Train Acc: 0.8424000000953674\n",
            "=== Epoch: 187 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.39534814761951564, Avg Train Acc: 0.8407999986410141\n",
            "=== Epoch: 188 ===\n",
            "100% 100/100 [01:21<00:00,  1.22it/s]\n",
            "Avg Train Loss: 0.40007076886482534, Avg Train Acc: 0.85519999563694\n",
            "=== Epoch: 189 ===\n",
            "100% 100/100 [01:22<00:00,  1.22it/s]\n",
            "Avg Train Loss: 0.447349216863513, Avg Train Acc: 0.8243999990820885\n",
            "=== Epoch: 190 ===\n",
            "100% 100/100 [01:22<00:00,  1.22it/s]\n",
            "Avg Train Loss: 0.4144763182103634, Avg Train Acc: 0.8340000003576279\n",
            "=== Epoch: 191 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.43516158789396286, Avg Train Acc: 0.8375999960303306\n",
            "=== Epoch: 192 ===\n",
            "100% 100/100 [01:24<00:00,  1.19it/s]\n",
            "Avg Train Loss: 0.41589085343293847, Avg Train Acc: 0.8283999985456467\n",
            "=== Epoch: 193 ===\n",
            "100% 100/100 [01:23<00:00,  1.19it/s]\n",
            "Avg Train Loss: 0.41124887447804215, Avg Train Acc: 0.8399999970197678\n",
            "=== Epoch: 194 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.4381196408346295, Avg Train Acc: 0.8332000023126602\n",
            "=== Epoch: 195 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.42168468561023476, Avg Train Acc: 0.8319999983906746\n",
            "=== Epoch: 196 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.40646062076091766, Avg Train Acc: 0.8395999979972839\n",
            "=== Epoch: 197 ===\n",
            "100% 100/100 [01:23<00:00,  1.20it/s]\n",
            "Avg Train Loss: 0.409786717351526, Avg Train Acc: 0.8275999969244003\n",
            "=== Epoch: 198 ===\n",
            "100% 100/100 [01:22<00:00,  1.22it/s]\n",
            "Avg Train Loss: 0.4817944648861885, Avg Train Acc: 0.8047999966144562\n",
            "=== Epoch: 199 ===\n",
            "100% 100/100 [01:22<00:00,  1.21it/s]\n",
            "Avg Train Loss: 0.420908711515367, Avg Train Acc: 0.8324000006914138\n",
            "Testing with last model..\n",
            "Test Acc: 0.7800399997532368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Without noise introduction, the best training accuracy achieved was 88.6%, and the best test accuracy obtained was 82.55%.\n",
        "---\n",
        "---\n",
        "\n",
        "## With noise introduction, the best training accuracy obtained is 83.24%, and the best test accuracy obtained in 78%.\n",
        "---"
      ],
      "metadata": {
        "id": "XS5o4TIMRnsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T6vFdKOUTHdN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
